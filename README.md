# TextGenerationWithGPT2\

# Overview

This Python program demonstrates text generation using the GPT-2 model from Hugging Face's transformers library. It also includes examples of working with word embeddings/vectors using the gensim library. The program explores NLP deep learning examples, such as text generation and word embeddings.

Getting Started
Prerequisites
Python 3.x
Dependencies: transformers, gensim
